"""
ä»»åŠ¡è°ƒåº¦å™¨ - å¤šçº¿ç¨‹æ¶æ„
1. æœç´¢çº¿ç¨‹ï¼šGitHub æœç´¢ API Keys
2. æ ¡éªŒçº¿ç¨‹ï¼šéªŒè¯ Keys æœ‰æ•ˆæ€§
3. åŒæ­¥çº¿ç¨‹ï¼šè‡ªåŠ¨åŒæ­¥æœ‰æ•ˆ Keys
4. é‡æ–°æ ¡éªŒçº¿ç¨‹ï¼šæ¯å¤©é‡æ–°éªŒè¯é™æµå¯†é’¥
"""
import os
import queue
import threading
import time
from datetime import datetime, timedelta
from typing import Dict, List

from common.Logger import logger
from common.config import config
from utils.db_manager import db_manager
from utils.sync_utils import sync_utils
from app.providers.config_based_factory import ConfigBasedAIProviderFactory


class TaskScheduler:
    """ä¸‰çº¿ç¨‹ä»»åŠ¡è°ƒåº¦å™¨"""

    def __init__(self):
        # ä»»åŠ¡é˜Ÿåˆ—
        self.search_queue = queue.Queue(maxsize=1000)  # æœç´¢ç»“æœé˜Ÿåˆ—ï¼ˆå¾…æ ¡éªŒï¼‰
        self.validation_queue = queue.Queue(maxsize=1000)  # æ ¡éªŒç»“æœé˜Ÿåˆ—ï¼ˆå¾…åŒæ­¥ï¼‰

        # çº¿ç¨‹æ§åˆ¶
        self.shutdown_flag = threading.Event()
        self.threads = []

    def start(self):
        """å¯åŠ¨æ‰€æœ‰çº¿ç¨‹"""
        # 1. æœç´¢çº¿ç¨‹
        search_thread = threading.Thread(
            target=self._search_worker,
            name="SearchWorker",
            daemon=True
        )
        search_thread.start()
        self.threads.append(search_thread)

        # 2. æ ¡éªŒçº¿ç¨‹ï¼ˆå¤šä¸ªå·¥ä½œçº¿ç¨‹ï¼‰
        for i in range(3):  # 3ä¸ªæ ¡éªŒçº¿ç¨‹å¹¶å‘
            validation_thread = threading.Thread(
                target=self._validation_worker,
                name=f"ValidationWorker-{i}",
                daemon=True
            )
            validation_thread.start()
            self.threads.append(validation_thread)

        # 3. åŒæ­¥çº¿ç¨‹
        sync_thread = threading.Thread(
            target=self._sync_worker,
            name="SyncWorker",
            daemon=True
        )
        sync_thread.start()
        self.threads.append(sync_thread)

        # 4. é™æµå¯†é’¥é‡æ–°æ ¡éªŒçº¿ç¨‹
        revalidation_thread = threading.Thread(
            target=self._revalidation_worker,
            name="RevalidationWorker",
            daemon=True
        )
        revalidation_thread.start()
        self.threads.append(revalidation_thread)

        # 5. åŒæ­¥ç›‘æ§çº¿ç¨‹ï¼ˆæ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡ï¼‰
        sync_monitor_thread = threading.Thread(
            target=self._sync_monitor_worker,
            name="SyncMonitorWorker",
            daemon=True
        )
        sync_monitor_thread.start()
        self.threads.append(sync_monitor_thread)

        logger.info("âœ… Task scheduler started with 4 worker types")

    def _search_worker(self):
        """æœç´¢çº¿ç¨‹ï¼šæ‰§è¡Œ GitHub æœç´¢ï¼ˆå“åº”å¼è°ƒåº¦ï¼‰"""
        from utils.github_client import GitHubClient
        from app.rate_limit_monitor import rate_limit_monitor
        import time

        github_client = GitHubClient(config.GITHUB_TOKENS)

        # æ³¨å†Œæ‰€æœ‰ tokens
        for token in config.GITHUB_TOKENS:
            rate_limit_monitor.register_token(token)

        # é¦–æ¬¡å¯åŠ¨ç«‹å³æ‰§è¡Œ
        first_run = True

        while not self.shutdown_flag.is_set():
            try:
                # è·å–æœç´¢æŸ¥è¯¢
                search_queries = self._get_search_queries()

                if not search_queries:
                    logger.warning("âš ï¸ No search queries, sleeping...")
                    time.sleep(300)
                    continue

                # å»é‡å…³é”®å­—ï¼ˆä»…é™æœ¬æ¬¡ä»»åŠ¡è¿è¡Œï¼‰
                original_count = len(search_queries)
                search_queries = list(dict.fromkeys(search_queries))  # ä¿æŒé¡ºåºçš„å»é‡
                deduplicated_count = len(search_queries)

                if original_count != deduplicated_count:
                    logger.info(f"ğŸ”„ Deduplicated queries: {original_count} â†’ {deduplicated_count} ({original_count - deduplicated_count} duplicates removed)")

                logger.info(f"ğŸ” Starting search with {len(search_queries)} unique queries")

                # è®°å½•æ‰§è¡Œå¼€å§‹æ—¶é—´å’Œç»Ÿè®¡
                start_time = time.time()
                total_search_requests = 0
                total_core_requests = 0
                files_processed = 0

                for i, query in enumerate(search_queries, 1):
                    if self.shutdown_flag.is_set():
                        break

                    try:
                        result = github_client.search_for_keys(query)

                        # æ›´æ–° rate limit monitor
                        if result and "rate_limit_info" in result and result["rate_limit_info"]:
                            rate_info = result["rate_limit_info"]
                            rate_limit_monitor.update_from_response(
                                token=rate_info['token'],
                                headers={
                                    'X-RateLimit-Remaining': rate_info['remaining'],
                                    'X-RateLimit-Limit': rate_info['limit'],
                                    'X-RateLimit-Reset': rate_info['reset'],
                                },
                                api_type='search'
                            )

                        # ç»Ÿè®¡è¯·æ±‚æ•°
                        if result and "stats" in result:
                            total_search_requests += result["stats"]["total_requests"]

                        if result and "items" in result:
                            for item in result["items"]:
                                # æ”¾å…¥æ ¡éªŒé˜Ÿåˆ—
                                self.search_queue.put({
                                    'item': item,
                                    'query': query
                                })
                                files_processed += 1

                        # API é™æµæ§åˆ¶
                        if i % 5 == 0:
                            time.sleep(2)

                    except Exception as e:
                        logger.error(f"Search error for query '{query}': {e}")

                # è®°å½•æ‰§è¡Œç»Ÿè®¡
                duration = time.time() - start_time
                rate_limit_monitor.record_search_execution(
                    queries_count=len(search_queries),
                    files_processed=files_processed,
                    search_requests=total_search_requests,
                    core_requests=total_core_requests,  # æ ¡éªŒçº¿ç¨‹ä¼šæ›´æ–°
                    duration_seconds=duration
                )

                # æ£€æŸ¥æ˜¯å¦å¯ç”¨åŠ¨æ€è°ƒåº¦
                use_dynamic = os.getenv("DYNAMIC_SCHEDULING", "true").lower() == "true"

                if use_dynamic:
                    # ğŸ¯ æ™ºèƒ½è°ƒåº¦ï¼šæ ¹æ®å®é™…æ¶ˆè€—è®¡ç®—ä¸‹æ¬¡é—´éš”
                    sleep_seconds = rate_limit_monitor.calculate_next_interval()
                    next_run = datetime.now() + timedelta(seconds=sleep_seconds)
                    logger.info(f"ğŸ’¤ Search complete, next run at {next_run.strftime('%Y-%m-%d %H:%M:%S')} "
                               f"(dynamic scheduling, interval: {sleep_seconds/60:.1f} min)")
                else:
                    # å›ºå®šæ—¶é—´è°ƒåº¦ï¼ˆæ”¯æŒç®€åŒ–é…ç½®ï¼‰
                    schedule_config = os.getenv("SCHEDULE_CRON", "3")
                    next_run, sleep_seconds = self._parse_schedule_config(schedule_config)
                    logger.info(f"ğŸ’¤ Search complete, next run at {next_run.strftime('%Y-%m-%d %H:%M:%S')} "
                               f"(fixed schedule: {schedule_config})")

                # åˆ†æ®µä¼‘çœ ï¼Œä»¥ä¾¿èƒ½å¤Ÿå“åº” shutdown ä¿¡å·
                while sleep_seconds > 0 and not self.shutdown_flag.is_set():
                    sleep_chunk = min(sleep_seconds, 60)
                    time.sleep(sleep_chunk)
                    sleep_seconds -= sleep_chunk

            except Exception as e:
                logger.error(f"Search worker error: {e}")
                time.sleep(60)

    def _validation_worker(self):
        """æ ¡éªŒçº¿ç¨‹ï¼šéªŒè¯ Keys"""
        from app.providers.config_key_extractor import ConfigKeyExtractor

        while not self.shutdown_flag.is_set():
            try:
                # ä»æœç´¢é˜Ÿåˆ—è·å–ä»»åŠ¡
                task = self.search_queue.get(timeout=30)

                item = task['item']
                file_sha = item.get("sha")
                file_url = item.get("html_url", "")
                file_path = item.get("path", "")
                repo_name = item["repository"]["full_name"]

                # æ£€æŸ¥æ˜¯å¦å·²æ‰«æï¼ˆåŸºäºSHAå»é‡ï¼‰
                if db_manager.is_file_scanned(file_sha):
                    continue

                # ä¸‹è½½æ–‡ä»¶å†…å®¹
                from utils.github_client import GitHubClient
                github_client = GitHubClient(config.GITHUB_TOKENS)
                content = github_client.get_file_content(item)

                if not content:
                    continue

                # æå–å¹¶éªŒè¯ Keys
                extracted = ConfigKeyExtractor.extract_all_keys(content)

                for provider_name, keys in extracted.items():
                    if not keys:
                        continue

                    provider = ConfigBasedAIProviderFactory.get_provider_by_name(provider_name)
                    if not provider:
                        continue

                    for key in keys:
                        validation_result = provider.validate_key(key)

                        if validation_result == "ok":
                            status = 'valid'
                            logger.info(f"âœ… VALID {provider_name.upper()}: {key[:20]}...")
                        elif "rate_limited" in validation_result:
                            status = 'rate_limited'
                            logger.warning(f"âš ï¸ RATE LIMITED {provider_name.upper()}: {key[:20]}...")
                        else:
                            status = 'invalid'
                            logger.info(f"âŒ INVALID {provider_name.upper()}: {key[:20]}...")

                        # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆä¸å†å­˜å‚¨ group_nameï¼ŒåŒæ­¥æ—¶å®æ—¶è·å–ï¼‰
                        key_id = db_manager.save_api_key(
                            api_key=key,
                            provider=provider_name,
                            status=status,
                            source_repo=repo_name,
                            source_file_path=file_path,
                            source_file_url=file_url,
                            source_file_sha=file_sha,
                            gpt_load_group_name=None,  # ä¸å­˜å‚¨ï¼ŒåŒæ­¥æ—¶å®æ—¶è·å–
                            metadata={'validation_result': validation_result}
                        )

                        # æœ‰æ•ˆçš„ Key æ”¾å…¥åŒæ­¥é˜Ÿåˆ—ï¼ˆå­˜å‚¨ provider_nameï¼ŒåŒæ­¥æ—¶åŠ¨æ€è·å– group_nameï¼‰
                        if status == 'valid' and key_id:
                            self.validation_queue.put({
                                'key_id': key_id,
                                'key': key,
                                'provider': provider_name  # åªå­˜å‚¨ providerï¼Œä¸å­˜å‚¨ group_name
                            })

                # æ ‡è®°æ–‡ä»¶å·²æ‰«æ
                db_manager.mark_file_scanned(
                    file_sha=file_sha,
                    repo=repo_name,
                    file_path=file_path,
                    file_url=file_url
                )

                self.search_queue.task_done()

            except queue.Empty:
                continue
            except Exception as e:
                import traceback
                logger.error(f"Validation worker error: {e}")
                logger.error(traceback.format_exc())

    def _sync_worker(self):
        """åŒæ­¥çº¿ç¨‹ï¼šè‡ªåŠ¨åŒæ­¥æœ‰æ•ˆ Keys"""
        while not self.shutdown_flag.is_set():
            try:
                # ä»æ ¡éªŒé˜Ÿåˆ—è·å–ä»»åŠ¡
                task = self.validation_queue.get(timeout=30)

                key_id = task['key_id']
                key = task['key']
                provider_name = task['provider']

                # å®æ—¶è·å– group_nameï¼ˆæ”¯æŒåŠ¨æ€ä¿®æ”¹é…ç½®ï¼‰
                from app.providers.config_key_extractor import ConfigKeyExtractor
                group_name = ConfigKeyExtractor.get_gpt_load_group_name(provider_name)

                # æ£€æŸ¥æ˜¯å¦é…ç½®äº† group_name
                if not group_name or not group_name.strip():
                    logger.warning(f"âš ï¸ Skipping key {key_id}: provider '{provider_name}' has no gpt_load_group_name configured")
                    self.validation_queue.task_done()
                    continue

                # æ‰§è¡ŒåŒæ­¥
                logger.info(f"ğŸ”„ Syncing key {key_id} (provider: {provider_name}) to group '{group_name}'...")

                result = sync_utils._send_gpt_load_worker([key], group_name)

                if result == "success":
                    db_manager.mark_key_synced(key_id, 'gpt_load', success=True)
                    logger.info(f"âœ… Synced key {key_id} to group '{group_name}'")
                else:
                    db_manager.mark_key_synced(key_id, 'gpt_load', success=False, error_message=result)
                    logger.error(f"âŒ Failed to sync key {key_id}: {result}")

                self.validation_queue.task_done()

                # åŒæ­¥é™æµ
                time.sleep(1)

            except queue.Empty:
                # é˜Ÿåˆ—ä¸ºç©ºæ—¶ï¼Œæ£€æŸ¥æ•°æ®åº“ä¸­çš„å¾…åŒæ­¥ Keys
                self._sync_pending_keys()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                logger.error(f"Sync worker error: {e}")

    def _sync_pending_keys(self):
        """åŒæ­¥æ•°æ®åº“ä¸­çš„å¾…åŒæ­¥ Keys"""
        try:
            pending_keys = db_manager.get_pending_sync_keys('gpt_load', limit=10)

            if not pending_keys:
                return

            logger.info(f"ğŸ”„ Found {len(pending_keys)} pending keys in database")

            for key_obj in pending_keys:
                try:
                    # å®æ—¶è·å– group_nameï¼ˆæ”¯æŒåŠ¨æ€ä¿®æ”¹é…ç½®ï¼‰
                    from app.providers.config_key_extractor import ConfigKeyExtractor
                    group_name = ConfigKeyExtractor.get_gpt_load_group_name(key_obj.provider)

                    # æ£€æŸ¥æ˜¯å¦é…ç½®äº† group_name
                    if not group_name or not group_name.strip():
                        logger.warning(f"âš ï¸ Skipping key {key_obj.id}: provider '{key_obj.provider}' has no gpt_load_group_name configured")
                        continue

                    from utils.crypto import key_encryption
                    decrypted_key = key_encryption.decrypt_key(key_obj.key_encrypted)

                    logger.info(f"ğŸ”„ Syncing pending key {key_obj.id} (provider: {key_obj.provider}) to group '{group_name}'...")

                    result = sync_utils._send_gpt_load_worker([decrypted_key], group_name)

                    if result == "success":
                        db_manager.mark_key_synced(key_obj.id, 'gpt_load', success=True)
                        logger.info(f"âœ… Synced pending key {key_obj.id} to group '{group_name}'")
                    else:
                        db_manager.mark_key_synced(key_obj.id, 'gpt_load', success=False, error_message=result)

                    time.sleep(1)

                except Exception as e:
                    logger.error(f"Failed to sync pending key {key_obj.id}: {e}")

        except Exception as e:
            logger.error(f"Failed to sync pending keys: {e}")

    def _parse_schedule_config(self, config: str) -> tuple:
        """
        è§£æè°ƒåº¦é…ç½®ï¼Œè¿”å› (next_run_time, sleep_seconds)

        æ”¯æŒæ ¼å¼ï¼š
        - "3" : æ¯å¤©å‡Œæ™¨3ç‚¹
        - "3,9,15,21" : æ¯å¤©å¤šä¸ªæ—¶é—´ç‚¹
        - "*/2" : æ¯2å°æ—¶
        """
        now = datetime.now()

        # ç®€å•å°æ—¶æ ¼å¼: "3" æˆ– "3,9,15,21"
        if ',' in config:
            # å¤šä¸ªæ—¶é—´ç‚¹
            hours = [int(h.strip()) for h in config.split(',')]
            hours.sort()

            # æ‰¾åˆ°ä¸‹ä¸€ä¸ªæ‰§è¡Œæ—¶é—´
            current_hour = now.hour
            next_hour = None

            for h in hours:
                if h > current_hour:
                    next_hour = h
                    break

            if next_hour is None:
                # ä»Šå¤©æ²¡æœ‰äº†ï¼Œç”¨æ˜å¤©çš„ç¬¬ä¸€ä¸ª
                next_hour = hours[0]
                next_run = (now + timedelta(days=1)).replace(hour=next_hour, minute=0, second=0, microsecond=0)
            else:
                next_run = now.replace(hour=next_hour, minute=0, second=0, microsecond=0)

        elif config.startswith('*/'):
            # æ¯Nå°æ—¶: "*/2"
            interval_hours = int(config[2:])
            next_run = now + timedelta(hours=interval_hours)
            next_run = next_run.replace(minute=0, second=0, microsecond=0)

        else:
            # å•ä¸ªå°æ—¶: "3"
            hour = int(config)
            next_run = now.replace(hour=hour, minute=0, second=0, microsecond=0)
            if next_run <= now:
                next_run += timedelta(days=1)

        sleep_seconds = (next_run - datetime.now()).total_seconds()
        return next_run, sleep_seconds

    def _get_search_queries(self):
        """è·å–æœç´¢æŸ¥è¯¢ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼Œæ•°æ®åº“æ¨¡å¼ï¼Œæ”¯æŒè‡ªå®šä¹‰å…³é”®å­—ï¼‰"""
        auto_queries = []
        providers = config.AI_PROVIDERS_CONFIG

        # ç®€åŒ–çš„è‡ªåŠ¨ç”Ÿæˆï¼ˆåªç”Ÿæˆæ ¸å¿ƒæŸ¥è¯¢ï¼‰
        languages = ['python', 'javascript', 'typescript', 'go']

        for provider in providers:
            patterns = provider.get('key_patterns', [])
            provider_name = provider.get('name', '').upper()
            custom_keywords = provider.get('custom_keywords', [])

            for pattern in patterns:
                import re
                if pattern.startswith('AIzaSy'):
                    prefix = 'AIzaSy'
                elif pattern.startswith('sk-'):
                    prefix = 'sk-'
                elif pattern.startswith('csk-'):
                    prefix = 'csk-'
                else:
                    match = re.match(r'^([A-Za-z0-9\-_]{3,10})', pattern)
                    if match:
                        prefix = match.group(1)
                    else:
                        continue

                # æ ¸å¿ƒæŸ¥è¯¢ï¼ˆé¢„è®¾ï¼‰
                for lang in languages:
                    auto_queries.append(f'"{provider_name}_API_KEY" = "{prefix}" language:{lang}')

                # è‡ªå®šä¹‰å…³é”®å­—æŸ¥è¯¢
                for custom_keyword in custom_keywords:
                    if custom_keyword and custom_keyword.strip():
                        for lang in languages:
                            auto_queries.append(f'"{custom_keyword}" "{prefix}" language:{lang}')

        return auto_queries

    def _revalidation_worker(self):
        """é™æµå¯†é’¥é‡æ–°æ ¡éªŒçº¿ç¨‹ï¼šæ¯å¤©è¿è¡Œä¸€æ¬¡"""
        from app.rate_limit_revalidator import rate_limit_revalidator

        # è·å–æ‰§è¡Œæ—¶é—´é…ç½®ï¼ˆé»˜è®¤æ¯å¤©å‡Œæ™¨ 2 ç‚¹ï¼‰
        revalidation_hour = int(os.getenv("REVALIDATION_HOUR", "2"))

        logger.info(f"ğŸ”„ Rate-limit revalidation worker started, will run daily at {revalidation_hour}:00")

        while not self.shutdown_flag.is_set():
            try:
                # è®¡ç®—ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´
                now = datetime.now()
                next_run = now.replace(hour=revalidation_hour, minute=0, second=0, microsecond=0)

                # å¦‚æœä»Šå¤©çš„æ‰§è¡Œæ—¶é—´å·²è¿‡ï¼Œæ¨è¿Ÿåˆ°æ˜å¤©
                if next_run <= now:
                    next_run += timedelta(days=1)

                # ç­‰å¾…åˆ°æ‰§è¡Œæ—¶é—´
                sleep_seconds = (next_run - datetime.now()).total_seconds()
                logger.info(f"ğŸ’¤ Next revalidation scheduled at {next_run.strftime('%Y-%m-%d %H:%M:%S')}")

                # åˆ†æ®µä¼‘çœ ï¼Œä»¥ä¾¿èƒ½å¤Ÿå“åº” shutdown ä¿¡å·
                while sleep_seconds > 0 and not self.shutdown_flag.is_set():
                    sleep_chunk = min(sleep_seconds, 60)  # æ¯æ¬¡æœ€å¤šç¡ 60 ç§’
                    time.sleep(sleep_chunk)
                    sleep_seconds -= sleep_chunk

                if self.shutdown_flag.is_set():
                    break

                # æ‰§è¡Œé‡æ–°æ ¡éªŒ
                logger.info("ğŸš€ Starting scheduled rate-limited keys revalidation")
                rate_limit_revalidator.revalidate_rate_limited_keys(batch_size=50)
                logger.info("âœ… Scheduled revalidation completed")

            except Exception as e:
                logger.error(f"âŒ Revalidation worker error: {e}")
                import traceback
                logger.error(traceback.format_exc())
                # å‡ºé”™åç­‰å¾… 1 å°æ—¶å†é‡è¯•
                time.sleep(3600)

    def _sync_monitor_worker(self):
        """åŒæ­¥ç›‘æ§çº¿ç¨‹ï¼šæ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡"""
        from app.sync_monitor import sync_monitor

        # é¦–æ¬¡å»¶è¿Ÿ15åˆ†é’Ÿå¯åŠ¨
        time.sleep(900)

        while not self.shutdown_flag.is_set():
            try:
                logger.info("ğŸ” Running sync monitor check...")
                sync_monitor.check_and_notify()

                # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
                if self.shutdown_flag.wait(timeout=3600):
                    break

            except Exception as e:
                logger.error(f"âŒ Sync monitor worker error: {e}")
                time.sleep(3600)  # å‘ç”Ÿé”™è¯¯åç­‰å¾…1å°æ—¶å†è¯•

    def shutdown(self):
        """åœæ­¢æ‰€æœ‰çº¿ç¨‹"""
        logger.info("ğŸ›‘ Shutting down task scheduler...")
        self.shutdown_flag.set()

        for thread in self.threads:
            thread.join(timeout=5)

        logger.info("âœ… Task scheduler stopped")


# å…¨å±€å®ä¾‹
task_scheduler = TaskScheduler()
