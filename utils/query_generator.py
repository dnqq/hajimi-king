import os
import re
from typing import List, Set, Dict
from common.config import config
from common.Logger import logger


class QueryGenerator:
    """æ ¹æ®ä¾›åº”å•†é…ç½®è‡ªåŠ¨ç”ŸæˆGitHubæœç´¢æŸ¥è¯¢"""

    @staticmethod
    def extract_search_prefix(pattern: str) -> str:
        """
        ä»æ­£åˆ™è¡¨è¾¾å¼ä¸­æå–å¯æœç´¢çš„å‰ç¼€

        Examples:
            AIzaSy[A-Za-z0-9\\-_]{33} -> AIzaSy
            sk-[A-Za-z0-9\\-_]{20,100} -> sk-
            sk-or-v1-[A-Za-z0-9\\-_]{20,100} -> sk-or-v1-
            csk-[A-Za-z0-9\\-_]{20,100} -> csk-

        Args:
            pattern: æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼

        Returns:
            str: æå–çš„å‰ç¼€ï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        # æ¸…ç†è½¬ä¹‰å­—ç¬¦
        pattern = pattern.replace('\\\\', '\\')

        # æå–å›ºå®šå‰ç¼€éƒ¨åˆ†ï¼ˆç›´åˆ°ç¬¬ä¸€ä¸ªæ­£åˆ™å…ƒå­—ç¬¦ï¼‰
        match = re.match(r'^([A-Za-z0-9\-_]+)', pattern)
        if match:
            prefix = match.group(1)
            # è‡³å°‘3ä¸ªå­—ç¬¦æ‰æœ‰æœç´¢æ„ä¹‰
            if len(prefix) >= 3:
                return prefix

        return ""

    @staticmethod
    def generate_queries_from_config() -> List[str]:
        """
        ä»ä¾›åº”å•†é…ç½®è‡ªåŠ¨ç”Ÿæˆæœç´¢æŸ¥è¯¢

        ä¸‰å±‚æŸ¥è¯¢ç­–ç•¥ï¼š
        1. ç²¾å‡†å‰ç¼€æŸ¥è¯¢ï¼ˆé’ˆå¯¹ç¨€æœ‰æ ¼å¼çš„ keyï¼‰
        2. é€šç”¨æ–‡ä»¶æŸ¥è¯¢ï¼ˆè¦†ç›–å¤šç§ key çš„å¸¸è§æ–‡ä»¶ï¼‰
        3. æ™ºèƒ½ç»„åˆæŸ¥è¯¢ï¼ˆæé«˜è¦†ç›–ç‡ï¼‰

        Returns:
            List[str]: è‡ªåŠ¨ç”Ÿæˆçš„æŸ¥è¯¢åˆ—è¡¨
        """
        queries = []
        all_prefixes = set()
        provider_prefix_map = {}  # è®°å½•æ¯ä¸ªå‰ç¼€å±äºå“ªä¸ªä¾›åº”å•†

        # === æ”¶é›†æ‰€æœ‰ä¾›åº”å•†çš„å‰ç¼€ ===
        for provider_config in config.AI_PROVIDERS_CONFIG:
            provider_name = provider_config.get('name', 'unknown')
            key_patterns = provider_config.get('key_patterns', [])

            for pattern in key_patterns:
                prefix = QueryGenerator.extract_search_prefix(pattern)

                if prefix and len(prefix) >= 3:
                    all_prefixes.add(prefix)
                    provider_prefix_map[prefix] = provider_name

        logger.info(f"ğŸ” Detected {len(all_prefixes)} unique key prefixes from {len(config.AI_PROVIDERS_CONFIG)} providers")
        for prefix, provider in provider_prefix_map.items():
            logger.info(f"   - {prefix} ({provider})")

        # === ç¬¬ä¸€å±‚ï¼šç²¾å‡†å‰ç¼€æŸ¥è¯¢ ===
        # ä¸ºæ¯ä¸ªå‰ç¼€ç”ŸæˆåŸºç¡€æœç´¢
        for prefix in sorted(all_prefixes):
            queries.append(f'"{prefix}" in:file')

        logger.info(f"âœ… Generated {len(all_prefixes)} prefix-based queries")

        # === ç¬¬äºŒå±‚ï¼šé€šç”¨é…ç½®æ–‡ä»¶æŸ¥è¯¢ ===
        # è¿™äº›æ–‡ä»¶é€šå¸¸åŒ…å«å¤šç§ API keyï¼Œä¸€æ¬¡æœç´¢å¯ä»¥æå–æ‰€æœ‰ä¾›åº”å•†çš„ key
        common_file_queries = [
            'filename:.env',
            'filename:.env.example',
            'filename:config.json',
            'filename:credentials.json',
            'filename:secrets.json',
            'path:config/ extension:json',
        ]
        queries.extend(common_file_queries)

        logger.info(f"âœ… Added {len(common_file_queries)} common file queries")

        # === ç¬¬ä¸‰å±‚ï¼šæ™ºèƒ½ç»„åˆæŸ¥è¯¢ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€è¦å¯ç”¨ï¼‰===
        # è¿™äº›æŸ¥è¯¢å¯ä»¥å‘ç°ä¸€äº›ä¸å¸¸è§çš„ä½ç½®
        # æ³¨é‡Šæ‰ä»¥é¿å…è¿‡å¤šæŸ¥è¯¢ï¼Œå¯ä»¥åœ¨ queries.txt ä¸­æ‰‹åŠ¨æ·»åŠ 
        # smart_queries = [
        #     'api_key in:file filename:.env',
        #     '"API_KEY=" in:file',
        #     'path:src/config/',
        # ]
        # queries.extend(smart_queries)

        logger.info(f"ğŸ¯ Total auto-generated queries: {len(queries)}")
        return queries

    @staticmethod
    def load_manual_queries(queries_file: str) -> List[str]:
        """
        åŠ è½½æ‰‹åŠ¨å®šä¹‰çš„æŸ¥è¯¢

        æ”¯æŒï¼š
        - ä»¥ # å¼€å¤´çš„æ³¨é‡Šè¡Œ
        - ç©ºè¡Œ
        - æ­£å¸¸çš„æŸ¥è¯¢è¯­å¥

        Args:
            queries_file: æŸ¥è¯¢æ–‡ä»¶è·¯å¾„

        Returns:
            List[str]: æ‰‹åŠ¨æŸ¥è¯¢åˆ—è¡¨
        """
        queries = []

        if not os.path.exists(queries_file):
            logger.info(f"ğŸ“‹ Manual queries file not found: {queries_file}")
            return queries

        try:
            with open(queries_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()

                    # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
                    if not line or line.startswith('#'):
                        continue

                    queries.append(line)

            logger.info(f"ğŸ“‹ Loaded {len(queries)} manual queries from {queries_file}")

        except Exception as e:
            logger.error(f"âŒ Failed to load manual queries from {queries_file}: {e}")

        return queries

    @staticmethod
    def normalize_query(query: str) -> str:
        """
        æ ‡å‡†åŒ–æŸ¥è¯¢ç”¨äºå»é‡

        å¤„ç†é€»è¾‘ï¼š
        1. ç§»é™¤å¤šä½™ç©ºæ ¼
        2. ä¿ç•™å¼•å·å†…å®¹çš„åŸå§‹é¡ºåº
        3. å¯¹éå¼•å·éƒ¨åˆ†è¿›è¡Œæ’åº

        Args:
            query: åŸå§‹æŸ¥è¯¢å­—ç¬¦ä¸²

        Returns:
            str: æ ‡å‡†åŒ–åçš„æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        query = " ".join(query.split())

        # åˆ†ç¦»å¼•å·å†…å®¹å’Œå…¶ä»–éƒ¨åˆ†
        parts = []
        i = 0
        while i < len(query):
            if query[i] == '"':
                # æ‰¾åˆ°é…å¯¹çš„å¼•å·
                end_quote = query.find('"', i + 1)
                if end_quote != -1:
                    parts.append(query[i:end_quote + 1])
                    i = end_quote + 1
                else:
                    parts.append(query[i])
                    i += 1
            elif query[i] == ' ':
                i += 1
            else:
                # æå–éå¼•å·çš„è¯
                start = i
                while i < len(query) and query[i] not in (' ', '"'):
                    i += 1
                parts.append(query[start:i])

        # å¯¹éƒ¨åˆ†è¿›è¡Œæ’åºï¼ˆå¼•å·å†…å®¹ä¿æŒåŸæ ·ï¼‰
        return " ".join(sorted(parts))

    @staticmethod
    def merge_queries(auto_queries: List[str], manual_queries: List[str]) -> List[str]:
        """
        åˆå¹¶è‡ªåŠ¨ç”Ÿæˆå’Œæ‰‹åŠ¨æŸ¥è¯¢ï¼Œå»é‡å¹¶ä¿ç•™æ‰‹åŠ¨æŸ¥è¯¢ä¼˜å…ˆçº§

        é€»è¾‘ï¼š
        1. æ‰‹åŠ¨æŸ¥è¯¢ä¼˜å…ˆï¼ˆæ”¾åœ¨å‰é¢ï¼‰
        2. è‡ªåŠ¨æŸ¥è¯¢è¡¥å……ï¼ˆå»é™¤ä¸æ‰‹åŠ¨é‡å¤çš„ï¼‰

        Args:
            auto_queries: è‡ªåŠ¨ç”Ÿæˆçš„æŸ¥è¯¢åˆ—è¡¨
            manual_queries: æ‰‹åŠ¨å®šä¹‰çš„æŸ¥è¯¢åˆ—è¡¨

        Returns:
            List[str]: åˆå¹¶åçš„æŸ¥è¯¢åˆ—è¡¨
        """
        seen = set()
        merged = []

        # ç»Ÿè®¡ä¿¡æ¯
        manual_count = 0
        auto_count = 0
        duplicate_count = 0

        # å…ˆæ·»åŠ æ‰‹åŠ¨æŸ¥è¯¢ï¼ˆæ ‡å‡†åŒ–åå»é‡ï¼‰
        for query in manual_queries:
            normalized = QueryGenerator.normalize_query(query)
            if normalized not in seen:
                seen.add(normalized)
                merged.append(query)
                manual_count += 1
            else:
                duplicate_count += 1

        # å†æ·»åŠ è‡ªåŠ¨æŸ¥è¯¢ï¼ˆè·³è¿‡é‡å¤çš„ï¼‰
        for query in auto_queries:
            normalized = QueryGenerator.normalize_query(query)
            if normalized not in seen:
                seen.add(normalized)
                merged.append(query)
                auto_count += 1
            else:
                duplicate_count += 1

        logger.info(f"âœ… Merged queries: {manual_count} manual + {auto_count} auto = {len(merged)} total")
        if duplicate_count > 0:
            logger.info(f"   â„¹ï¸  Skipped {duplicate_count} duplicate queries")

        return merged

    @staticmethod
    def get_query_statistics(queries: List[str]) -> Dict[str, int]:
        """
        ç»Ÿè®¡æŸ¥è¯¢çš„ç±»å‹åˆ†å¸ƒ

        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨

        Returns:
            Dict[str, int]: ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            'total': len(queries),
            'prefix_based': 0,
            'filename_based': 0,
            'path_based': 0,
            'content_based': 0,
            'other': 0
        }

        for query in queries:
            query_lower = query.lower()

            if 'in:file' in query_lower and '"' in query:
                stats['prefix_based'] += 1
            elif 'filename:' in query_lower:
                stats['filename_based'] += 1
            elif 'path:' in query_lower:
                stats['path_based'] += 1
            elif any(keyword in query_lower for keyword in ['api_key', 'secret', 'token']):
                stats['content_based'] += 1
            else:
                stats['other'] += 1

        return stats


# å…¨å±€å®ä¾‹
query_generator = QueryGenerator()
